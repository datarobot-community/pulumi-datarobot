// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../types/input";
import * as outputs from "../types/output";

export interface ApplicationSourceResourceSettings {
    /**
     * The replicas for the Application Source.
     */
    replicas: number;
}

export interface ApplicationSourceRuntimeParameterValue {
    /**
     * The name of the runtime parameter.
     */
    key: string;
    /**
     * The type of the runtime parameter.
     */
    type: string;
    /**
     * The value of the runtime parameter (type conversion is handled internally).
     */
    value: string;
}

export interface CustomModelGuardConfiguration {
    /**
     * The deployment ID of this guard.
     */
    deploymentId?: string;
    /**
     * The input column name of this guard.
     */
    inputColumnName?: string;
    /**
     * The intervention for the guard configuration.
     */
    intervention: outputs.CustomModelGuardConfigurationIntervention;
    /**
     * The LLM type for this guard.
     */
    llmType?: string;
    /**
     * The name of the guard configuration.
     */
    name: string;
    /**
     * The OpenAI API base URL for this guard.
     */
    openaiApiBase?: string;
    /**
     * The ID of an OpenAI credential for this guard.
     */
    openaiCredential?: string;
    /**
     * The ID of an OpenAI deployment for this guard.
     */
    openaiDeploymentId?: string;
    /**
     * The output column name of this guard.
     */
    outputColumnName?: string;
    /**
     * The list of stages for the guard configuration.
     */
    stages: string[];
    /**
     * The template name of the guard configuration.
     */
    templateName: string;
}

export interface CustomModelGuardConfigurationIntervention {
    /**
     * The action of the guard intervention.
     */
    action: string;
    /**
     * The list of conditions for the guard intervention.
     */
    condition: outputs.CustomModelGuardConfigurationInterventionCondition;
    /**
     * The message of the guard intervention.
     */
    message: string;
}

export interface CustomModelGuardConfigurationInterventionCondition {
    /**
     * The comparand of the guard condition.
     */
    comparand: number;
    /**
     * The comparator of the guard condition.
     */
    comparator: string;
}

export interface CustomModelOverallModerationConfiguration {
    /**
     * The timeout action of the overall moderation configuration.
     */
    timeoutAction: string;
    /**
     * The timeout in seconds of the overall moderation configuration.
     */
    timeoutSec: number;
}

export interface CustomModelResourceSettings {
    /**
     * The memory in MB for the Custom Model.
     */
    memoryMb: number;
    /**
     * The network access for the Custom Model.
     */
    networkAccess: string;
    /**
     * The replicas for the Custom Model.
     */
    replicas: number;
}

export interface CustomModelRuntimeParameterValue {
    /**
     * The name of the runtime parameter.
     */
    key: string;
    /**
     * The type of the runtime parameter.
     */
    type: string;
    /**
     * The value of the runtime parameter (type conversion is handled internally).
     */
    value: string;
}

export interface CustomModelSourceRemoteRepository {
    /**
     * The ID of the source remote repository.
     */
    id: string;
    /**
     * The reference of the source remote repository.
     */
    ref: string;
    /**
     * The list of source paths in the source remote repository.
     */
    sourcePaths: string[];
}

export interface DeploymentSettings {
    /**
     * Used to associate predictions back to your actual data.
     */
    associationId?: outputs.DeploymentSettingsAssociationId;
    /**
     * Used to compare the performance of the deployed model with the challenger models.
     */
    challengerAnalysis?: boolean;
    /**
     * Used to score predictions made by the challenger models and compare performance with the deployed model.
     */
    predictionRowStorage?: boolean;
    /**
     * Settings for the predictions.
     */
    predictionsSettings?: outputs.DeploymentSettingsPredictionsSettings;
}

export interface DeploymentSettingsAssociationId {
    /**
     * Whether to automatically generate an association ID.
     */
    autoGenerateId: boolean;
    /**
     * The name of the feature to use as the association ID.
     */
    featureName: string;
    /**
     * Whether the association ID is required in prediction requests.
     */
    requiredInPredictionRequests: boolean;
}

export interface DeploymentSettingsPredictionsSettings {
    /**
     * The maximum number of computes to use for predictions.
     */
    maxComputes: number;
    /**
     * The minimum number of computes to use for predictions.
     */
    minComputes: number;
    /**
     * Whether to use real-time predictions.
     */
    realTime: boolean;
}

export interface LlmBlueprintLlmSettings {
    /**
     * The maximum number of tokens allowed in the completion. The combined count of this value and prompt tokens must be below the model's maximum context size, where prompt token count is comprised of system prompt, user prompt, recent chat history, and vector database citations.
     */
    maxCompletionLength?: number;
    /**
     * Guides the style of the LLM response. It is a 'universal' prompt, prepended to all individual prompts.
     */
    systemPrompt?: string;
    /**
     * Controls the randomness of model output, where higher values return more diverse output and lower values return more deterministic results.
     */
    temperature?: number;
    /**
     * Threshold that controls the selection of words included in the response, based on a cumulative probability cutoff for token selection. Higher numbers return more diverse options for outputs.
     */
    topP?: number;
}

export interface LlmBlueprintVectorDatabaseSettings {
    /**
     * The maximum number of documents to retrieve from the Vector Database.
     */
    maxDocumentsRetrievedPerPrompt?: number;
    /**
     * The maximum number of tokens to retrieve from the Vector Database.
     */
    maxTokens?: number;
}

export interface VectorDatabaseChunkingParameters {
    /**
     * The percentage of overlap between chunks.
     */
    chunkOverlapPercentage: number;
    /**
     * The size of the chunks.
     */
    chunkSize: number;
    /**
     * The method used to chunk the data.
     */
    chunkingMethod: string;
    /**
     * The id of the Embedding Model.
     */
    embeddingModel: string;
    /**
     * Whether the separator is a regex.
     */
    isSeparatorRegex: boolean;
    /**
     * The separators used to split the data.
     */
    separators: string[];
}

