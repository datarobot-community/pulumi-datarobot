// *** WARNING: this file was generated by pulumi. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;
using Pulumi;

namespace DataRobotPulumi.Datarobot.Outputs
{

    [OutputType]
    public sealed class LlmBlueprintLlmSettings
    {
        /// <summary>
        /// The maximum number of tokens allowed in the completion. The combined count of this value and prompt tokens must be below the model's maximum context size, where prompt token count is comprised of system prompt, user prompt, recent chat history, and vector database citations.
        /// </summary>
        public readonly int? MaxCompletionLength;
        /// <summary>
        /// Guides the style of the LLM response. It is a 'universal' prompt, prepended to all individual prompts.
        /// </summary>
        public readonly string? SystemPrompt;
        /// <summary>
        /// Controls the randomness of model output, where higher values return more diverse output and lower values return more deterministic results.
        /// </summary>
        public readonly double? Temperature;
        /// <summary>
        /// Threshold that controls the selection of words included in the response, based on a cumulative probability cutoff for token selection. Higher numbers return more diverse options for outputs.
        /// </summary>
        public readonly double? TopP;

        [OutputConstructor]
        private LlmBlueprintLlmSettings(
            int? maxCompletionLength,

            string? systemPrompt,

            double? temperature,

            double? topP)
        {
            MaxCompletionLength = maxCompletionLength;
            SystemPrompt = systemPrompt;
            Temperature = temperature;
            TopP = topP;
        }
    }
}
